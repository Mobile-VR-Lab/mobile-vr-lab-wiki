# **User Roles / Personas**

## Role: Student

* **_Motivations/Goals:_**
  * _VR interaction without controllers:_
    * **Goal:** Navigate the VR environment intuitively without relying on controllers.
    * **Action:** Utilize hand gestures to interact with the VR headset.
  * _Independent learning:_
    * **Goal:** Gain in-depth knowledge about the exhibit at a personalized pace.
    * **Action**: Have the freedom to explore each room/scene independently.
  * _Eye comfort and safety:_
    * **Goal:** Prevent eye strain.
    * **Action:** Easily adjust the headset’s rendering settings, including pupil distance.
  * _Distraction-Free Engagement:_
    * **Goal:** Engage with the VR application without unnecessary interruptions.
    * **Action:** Ensure the application initializes instantly upon wearing the headset, providing a smooth transition into the learning environment.
  * _Maintain personal space boundaries:_
    * **Goal:** Engage in the VR experience without causing physical distraction or harm to peers.
    * **Action:** Be presented with virtual boundaries that are configured to reflect any limitations of the physical environment.
* **_Student Persona #1:_** **Jamie**
  * _Description:_
    * **Age:** Middle school student.
    * **Experience with VR:** Limited.
    * **Characteristics:** Eager to learn, open to new experiences, and has an interest in immersive technologies. Jamie may need some initial guidance because of his limited experience with VR, so he anticipates a supportive introduction to using the technology effectively.
* **_Student Persona #2:_** **Alexia**
  * _Description:_
    * **Age:** Middle school student.
    * **Experience with VR:** Proficient.
    * **Characteristics:** Since her family owns a Quest headset, Alexia is very familiar with navigating VR technology and is excited to explore marine exhibits in a virtual space. Alexia anticipates a rich and informative experience that aligns with her interests and technical savviness.

## Role: Educational Instructor / Teacher

* **_Motivations/Goals:_**
  * _Scene awareness:_
    * **Goal:** Understand the current scene students are engaged with to provide relevant information and guidance.
    * **Action**: View the active scene on the instructor app, with scene information displayed.
  * _Smooth transitions:_
    * **Goal:** Facilitate seamless movement of students between different VR scenes for a coordinated learning experience.
    * **Action:** Use the instructor app to control the transition of all student headsets between various scenes.
  * _Attention management:_
    * **Goal:** Capture students' attention during announcements or instructions efficiently without physical interruptions.
    * **Action:** Activate an attention mode through the app that temporarily restricts students' VR exploration, focusing their attention on the instructor's directives.
  * _Guided exploration:_
    * **Goal:** Provide students with guided information.
    * **Action:** Have access to and read from scripts in the app pertaining to the current scene and its exhibits.
  * _Interactive demonstrations:_
    * **Goal:** Engage students through interactive exhibit features during the lesson.
    * **Action:** Trigger exhibit-specific actions and highlights through the instructor app to emphasize learning points.
  * _Headset Monitoring:_
    * **Goal:** Identify and resolve connectivity issues immediately as they arise.
    * **Action:** View and be notified of real-time headset status on the instructor app.
  * _Isolation Control:_
    * **Goal:** Reconnect individual student headsets without disrupting the entire class's VR experience in case of technical issues.
    * **Action:** Isolate and manage connectivity for individual headsets through the app to maintain a continuous learning experience for others.
  * _Safe Navigation:_
    * **Goal:** Ensure students can navigate safely in the classroom during the VR session.
    * **Action:** Enable transparency mode for students if needed, allowing them to be aware of their real-life surroundings when necessary.
* **_Instructor Persona #1:_** **Mrs. Smith**
  * _Description:_
    * **Age:** 54
    * **Experience with Tech:** Moderate; comfortable using standard educational tech tools but new to VR technology.
    * **Characteristics:** Dedicated educator enthusiastic about integrating new tech to enhance student learning. Requires a system that is straightforward and does not have a steep learning curve.
* **_Instructor Persona #2:_** **Mr Johnson**
  * _Description:_
    * **Age:** 29
    * **Experience with Tech:** Tech-savvy, has used VR in informal settings.
    * **Characteristics:** Young, energetic educator who is excited to incorporate VR tech solutions in the classroom. Quick to adapt to new technology but values hassle-free operations.

## Role: Content Creator

* **_Motivations / Goals:_**
  * _Scene development:_
    * **Goal:** Develop and implement new scenes for various educational topics.
    * **Action:** Utilize a user-friendly interface or platform where new scenes can be crafted, edited, and integrated into the existing VR educational environment.
  * _Exhibit interaction enhancement:_
    * **Goal:** Add new interactive actions to enrich user engagement with specific exhibits within the VR scenes.
    * **Action:** Implement new interactive functions or actions that can be easily assigned to specific exhibits, enhancing the educational value and user experience.
  * _Scriptwriting:_
    * **Goal:** Provide educators with engaging and informative scripts tailored to each scene and exhibit.
    * **Action:** Write and upload new scripts that instructors can access and use during the VR sessions, ensuring consistency and relevance with the corresponding scenes and exhibits.
  * _Asset integration:_
    * **Goal:** Efficiently add and manage unique assets within the VR environment.
    * **Action:** Upload and assign unique IDs to new assets, allowing for seamless integration and reference within scripts and the overall exhibit system.
  * _Schema utilization:_
    * **Goal:** Simplify the process of creating configurations for the software stack.
    * **Action:** Employ a well-defined schema that facilitates easy configuration creation, ensuring compatibility and ease of integration with the existing software stack.
  * _Deployment management:_
    * **Goal:** Streamline the deployment of updates and new content to the VR lab hardware.
    * **Action:** Use a centralized management system or platform that allows for efficient rolling out of updates, new scenes, and other content to the lab hardware without disrupting the user experience.
* **_Content Creator Persona #1:_** **Taylor**
  * _Description:_
    * **Age:** 35
    * **Experience with Tech:** Advanced; has experience in VR content creation and understands the technical requirements of the field.
    * **Characteristics:** Focused and passionate about delivering engaging educational content. Taylor has a strong understanding of both the creative and technical aspects involved in content creation.
* **_Content Creator Persona #2:_** **Jordan**
  * _Description:_
    * **Age:** 28
    * **Experience with Tech:** Moderate; comes from a background in education and has recently transitioned into tech-based educational content creation.
    * **Characteristics:** Enthusiastic and innovative, Jordan is always exploring new ways to make learning exciting and impactful through technology. Though not a tech expert, Jordan is a quick learner and is dedicated to mastering the necessary tools and platforms to deliver VR content.

# **Stories**

* Student User Stories: 
  1. As a student, I want to use hand gestures to navigate the VR environment so that I can interact naturally without the need for controllers.
  2. As a student, I want to be presented with virtual boundaries within the VR environment so that I avoid causing physical distraction or harm to my peers. 
  3. As a student, I want to easily switch between different languages within the VR environment, allowing me to engage with content in my preferred or native language.
  4. As a student, I want to be informed about the importance of adjusting the pupil distance in the VR headset so that I understand its impact on my viewing comfort and eye strain.
  5. As a student, I want a user-friendly interface to navigate through various VR scenes and exhibits, minimizing the learning curve and enhancing my engagement with the content.


* Instructor User Stories: 
  1. As an instructor, I want to view the current scene my students are exploring in real-time so that I can provide contextually relevant information and guidance.
  2. As an instructor, I want to control and navigate the virtual scenes that my students are exposed to during the lesson so that I can facilitate a synchronized educational experience for the entire group.
  3. As an instructor, I want a reliable and immediate way to capture all students' attention through their VR headsets during the lesson so that they are ready to receive important information or instructions.
  4. As an instructor, I want to dynamically load scripts relevant to the current VR exhibits so that I can provide accurate and engaging information without manually searching for details.
  5. As an instructor, I want the ability to interactively engage with the exhibit by triggering predetermined actions via clickable elements within the script, allowing me to emphasize specific parts of the exhibit and enhance the students’ learning experience.
  6. As an instructor, I want to see real-time connectivity statuses of each student’s headset so that I can promptly identify and address any disconnection issues and ensure that other students remain engaged in the VR experience.
  7. As an instructor, I want to toggle a transparency (pass-through) mode on student headsets to allow them to briefly see their real-world environment so that they are aware of their surroundings when necessary.


* Content Creator User Stories: 
  1. As a content creator, I want to use a well-defined schema to simplify the process of creating configurations for the software stack.
  2. As a content creator, I want the ability to preview and test the VR content I create in real-time, ensuring it functions as intended before deployment.
  3. As a content creator, I want a centralized repository to store, organize, and manage all assets, scripts, and configurations related to the VR content, facilitating efficient content updates and maintenance.
  4. As a content creator, I want templates and guidelines provided within the content creation platform to assist in crafting content that aligns with educational standards and best practices, ensuring the VR content is effective and pedagogically sound.
  5. As a content creator, I want to seamlessly integrate the newly created scenes into the existing VR educational environment without causing conflicts or disruptions, providing a fluid and coherent user experience.

# **Expanded Stories**

## Expanded Student User Stories: 

### **_1\. As a student, I want to use hand gestures to navigate the VR environment so that I can interact naturally without the need for controllers._**

* _Definition of Done:_ 
  * The VR application recognizes and accurately responds to a predefined set of hand gestures that facilitate navigation, selection, and interaction within the virtual space.
  * Students can smoothly transition between different sections or modules of the educational content using gestures without experiencing lag or errors.
  * The system provides feedback to the students confirming that their gestures have been detected and processed.
  * The gesture-based control system is tested and validated with a diverse group of individuals, ensuring its accessibility.
* _Tasks:_
  1. Research VR hand gestures in both Unity and Unreal Engine.
  2. Implement and configure a gesture recognition library within the VR application.
  3. Define and document a set of gestures that will be used for navigation and interaction, ensuring they are easy to perform and remember.
  4. Develop a feedback system within the application that acknowledges gesture inputs through visual and auditory cues.
  5. Conduct usability testing sessions with Quest 3 headsets to refine and optimize the gesture-based interaction system.
  6. Develop a brief tutorial or guide that introduces students to the gesture controls, helping them get acquainted with the system before engaging with the main educational content.

### **_2\. As a student, I want to be presented with virtual boundaries within the VR environment so that I avoid causing physical distraction or harm to my peers._** 

* _Definition of Done:_ 
  * The VR application dynamically generates visible and intuitive virtual boundaries that correspond accurately to the physical limitations of the room or designated VR area.
  * Students receive real-time visual cues or warnings as they approach these virtual boundaries, preventing unintentional disruptions or collisions.
  * The virtual boundary system is responsive and reliable, not interfering with the primary educational content but providing a safeguard against potential space management issues.
  * The system has been tested and validated for various physical spaces and with different student age groups to ensure it is effective and doesn't hinder the learning experience.
  * There should be documentation for instructors and students explaining how the virtual boundary system operates.
* _Tasks:_
  1. Research and select an appropriate method for creating dynamic virtual boundaries within the VR experience.
  2. Develop an algorithm or utilize an existing Unity library that can accurately translate the physical dimensions and limitations of a space into the virtual environment.
  3. Design visual cues or indicators within the VR application that clearly signify the virtual boundaries to the students. These should be easily distinguishable but not disruptive to the learning experience.
  4. Implement a feedback mechanism that alerts students as they approach or cross these virtual boundaries. This feedback should be immediate and noticeable without causing undue alarm or distraction.
  5. Conduct extensive testing of the virtual boundary system in different physical spaces to ensure its accuracy and reliability. Adjust the algorithm or visual cues as necessary based on test results.
  6. Develop and integrate a calibration or setup tool that allows instructors or administrators to easily configure the virtual boundaries based on the specific characteristics of their physical learning environment.
  7. Prepare instructional material or guides that explain to instructors and students how to use and understand the virtual boundary system effectively.
  8. Solicit feedback from both students and instructors after initial implementation, making necessary adjustments to improve user experience and system accuracy.
  9. Conduct a final round of testing and validation with end-users (students and instructors) to confirm that the system meets the established criteria for success and user satisfaction.

## Expanded Instructor User Stories: 

### **_3\. As an instructor, I want to view the current scene my students are exploring in real-time so that I can provide contextually relevant information and guidance._**

* _Definition of Done:_ 
  * The instructor app reliably displays the active scene that students are currently engaged with, updating in real time without significant delay.
  * Scene information is presented in a clear, readable format, showing not only the scene number or title but also a brief description or thumbnail image that helps the instructor quickly understand the content of the scene.
  * The displayed scene information on the instructor app is synchronized with the server application to ensure accuracy and consistency across the system.
  * The app has undergone thorough testing to guarantee it performs accurately under various network conditions and with different numbers of connected student headsets.
  * Documentation is provided to guide instructors on how to use the feature effectively.
* _Tasks:_
  1. Define the data structure for scenes, ensuring that each scene has a unique identifier, title, and optionally a short description or thumbnail.
  2. Implement a real-time updating mechanism on the server application that keeps track of the current scene displayed on each student’s headset.
  3. Develop a feature on the server application that broadcasts the current active scene information to the instructor app.
  4. On the instructor app, implement a user interface component (e.g., text box, image display area) that presents the received scene information in a clear and accessible manner.
  5. Design and implement error-handling mechanisms to address scenarios where scene information might not be available or is delayed, ensuring the instructor app remains stable and usable.
  6. Conduct unit testing on the new features implemented on both the server application and instructor app to catch and rectify potential bugs or issues.
  7. Perform integration testing to ensure the real-time scene information feature works seamlessly and accurately when student headsets, the server application, and the instructor app interact in a live environment.
  8. Develop user documentation that explains to instructors how to interpret and use the real-time scene information feature on the instructor app.
  9. Collect feedback from instructors during the testing phase and make necessary adjustments to improve the user interface and user experience of the real-time scene information feature on the instructor app.

### **_4\. As an instructor, I want to control and navigate the virtual scenes that my students are exposed to during the lesson so that I can facilitate a synchronized educational experience for the entire group._**

* _Definition of Done:_ 
  * The instructor has a user-friendly interface on the Android app to navigate between different scenes effortlessly.
  * A seamless transition occurs in students’ VR headsets when the instructor decides to change scenes, with minimal latency.
  * The app provides visual confirmation to the instructor once the scene transition is successful on all connected student headsets.
  * The scene control feature is robust and reliable, working consistently under various conditions without crashing or causing glitches.
  * The instructor has the ability to both advance to the next scene and return to the previous scene, offering flexibility in conducting the lesson.
  * Feature has undergone testing with actual users (instructors), and they find it intuitive and easy to use without extensive training or technical know-how.
  * Documentation and user support materials are available to guide instructors on using the scene navigation feature efficiently.
* _Tasks:_
  1. Define and implement the backend logic on the server application to handle requests for scene changes, ensuring synchronized transitions on all connected headsets.
  2. Design and develop a clean and intuitive UI element on the Android app for scene navigation, including 'Next' and 'Previous' buttons or swipe gestures.
  3. Implement functionality in the Android app to send scene navigation requests to the server application upon instructor interaction.
  4. Develop a mechanism on the server application to broadcast scene change commands to all connected student headsets.
  5. On student headset applications, implement a feature to receive and process scene change commands from the server, initiating a smooth transition to the designated scene.
  6. Implement a confirmation system where the server receives acknowledgment from each headset upon successful scene transition, and this status is relayed back to the instructor app.
  7. Conduct extensive testing to ensure the entire scene navigation system works seamlessly, addressing any bugs, latency issues, or synchronization problems identified during testing.
  8. Develop user-friendly documentation and support materials to guide instructors on effectively using the scene navigation feature during lessons.
  9. Collect and incorporate feedback from actual instructors during the testing and deployment phase to enhance the feature’s usability and effectiveness in real-world educational settings.

### **_5\. As an instructor, I want a reliable and immediate way to capture all students' attention through their VR headsets during the lesson so that they are ready to receive important information or instructions._**

* _Definition of Done:_ 
  * The instructor can easily activate the "attention mode" using a prominently displayed, intuitive button or control on the Android app.
  * Upon activation, students’ ability to interact with or navigate the virtual environment is temporarily suspended, drawing their focus to the instructor.
  * Students receive a visual or auditory notification in their headsets indicating that the instructor requires their attention.
  * The “attention mode” activation and deactivation process is smooth and immediate, with no noticeable lag or delay.
  * The feature works reliably and consistently across different scenarios and doesn’t interfere with the functioning of other app features or the overall VR experience.
  * Feedback from instructors indicates satisfaction with the feature’s ease of use and effectiveness in capturing students' attention during lessons.
* _Tasks:_
  1. Design and integrate a user-friendly "attention mode" toggle button on the Android app's instructor interface. Consider using a distinctive icon or color to ensure visibility.
  2. Develop backend functionality on the server application to receive “attention mode” activation requests from the Android app and broadcast corresponding commands to all connected student headsets.
  3. Implement a responsive feature in the student headset application that reacts to “attention mode” commands by temporarily disabling interactive elements and navigation within the virtual scene.
  4. Integrate visual and/or auditory signals within the student headset application to notify students clearly when “attention mode” is activated, guiding them to focus on the instructor.
  5. Ensure the student headset application restores full interactive capabilities smoothly and promptly once “attention mode” is deactivated.
  6. Conduct rigorous testing of the “attention mode” feature to identify and rectify any bugs, latency issues, or inconsistencies in behavior across different devices and network conditions.
  7. Seek feedback from instructors during testing and initial deployment phases, making necessary adjustments to improve the feature’s usability and effectiveness based on their input.
  8. Prepare concise and clear instructional materials or guides to assist instructors in utilizing the “attention mode” feature efficiently during their lessons.

### **_6\. As an instructor, I want to dynamically load scripts relevant to the current VR exhibits so that I can provide accurate and engaging information without manually searching for details._**

* _Definition of Done:_ 
  * Instructors can easily view a pre-written, editable script through the Android app, which corresponds to the active VR exhibit being engaged by students.
  * Script transition is automatic and instantaneous when the active VR exhibit changes, requiring no manual intervention from the instructor.
  * The script text is clear, legible, and presented in a user-friendly format, allowing the instructor to read comfortably without distraction.
  * The script loading mechanism is reliable, with minimal to zero lag or errors during dynamic content transitions.
  * Instructor feedback confirms that the feature enhances their teaching experience and aids in delivering informative content about VR exhibits to students.
* _Tasks:_
  1. Define a standardized format for script files (consider XML, JSON, or markdown), ensuring it supports varied content types, is easy to edit, and is efficiently parsable.
  2. Design a system on the server-side to map script files to their corresponding VR exhibits, allowing for quick retrieval and dynamic loading when exhibit scenes change.
  3. Implement a mechanism within the Android app to fetch and display the appropriate script based on the active VR exhibit. This might involve real-time communication with the server to receive the right script.
  4. Integrate a responsive and user-friendly text display component within the Android app, ensuring it provides a comfortable reading experience for instructors with various text size adjustments and readability features.
  5. Establish an error-handling and notification system to alert instructors if script loading fails or encounters issues, offering alternatives or prompts to reload.
  6. Conduct comprehensive testing of the script-loading feature across various scenarios and exhibits to ensure stability, accuracy, and reliability during live instructional sessions.
  7. Seek continuous feedback from instructors using the feature and make iterative improvements to the script display interface and loading mechanism based on their suggestions and reported challenges.
  8. Develop clear documentation and instructional guides for instructors explaining the use of the script-loading and display feature, assisting them in seamlessly integrating this tool into their teaching process.

### **_7\. As an instructor, I want the ability to interactively engage with the exhibit by triggering predetermined actions via clickable elements within the script, allowing me to emphasize specific parts of the exhibit and enhance the students’ learning experience._**

* _Definition of Done:_ 
  * Within the app’s script display, certain words or phrases are interactively clickable, serving as triggers for specific actions within the VR exhibit.
  * Clickable elements within the script are distinct and easily recognizable, ensuring the instructor can identify and use them without confusion.
  * Upon clicking an interactive element, the corresponding action is immediately reflected in the VR exhibit, such as highlighting, zooming, or presenting additional information.
  * The triggered actions in the VR exhibit are visible to all students concurrently, providing a shared, enhanced learning experience.
  * The system supports multiple types of actions that can be predefined and assigned to clickable script elements, offering flexibility in designing interactive VR exhibits.
* _Tasks:_
   1. Determine and define a variety of possible actions that can be triggered within the VR exhibits.
   2. Develop a syntax or tagging system within the script files to associate specific words or phrases with predetermined actions.
   3. Implement a script parser in the Android app that can identify and appropriately render interactive elements within the script text as clickable.
   4. Create action mapping: Develop a mapping mechanism that associates each clickable script element with its corresponding action within the VR exhibit.
   5. Establish an action communication protocol between the Android app and the VR system to effectively transmit action triggers in real-time.
   6. Within the VR system, implement responses to received action triggers that modify the exhibit as predefined (e.g., highlight, zoom, display additional info).
   7. Design and implement a user-friendly interface within the app that clearly presents interactive script elements without disrupting the readability or flow of the text.
   8. Conduct thorough testing of the interactive script functionality, ensuring reliable performance, accurate action triggering, and user-friendly interaction.
   9. Prepare documentation and guidelines for instructors on how to use the interactive script functionality effectively during their sessions.
  10. Collect feedback from instructors and observe the system in action, making continuous improvements and updates to enhance the interactive learning experience.
  11. Ensure that the interactive script feature is compatible with various VR exhibits and does not interfere with other functionalities of the VR learning system.

### **_8\. As an instructor, I want to see real-time connectivity statuses of each student’s headset so that I can promptly identify and address any disconnection issues and ensure that other students remain engaged in the VR experience._**

* _Definition of Done:_ 
  * The instructor app effectively displays a real-time status of each headset, showing whether it's connected, disconnected, or experiencing issues.
  * The status indicators are intuitive, easily understandable, and promptly updated to reflect changes in connectivity.
  * In case of headset disconnection, the app provides necessary options or guidance to the instructor for troubleshooting or reconnecting the device.
  * The functionality is robust and reliable, allowing the instructor to focus on teaching without constant concern over technical interruptions.
* _Tasks:_
  1. Implement a mechanism on the server-side that continuously monitors the connectivity status of each headset, detecting any disconnections or issues promptly.
  2. Design and develop a set of intuitive, easily recognizable status indicators to be displayed on the instructor app, each representing different connectivity states (e.g., connected, disconnected, reconnecting).
  3. Integrate the status indicators into the app’s interface, ensuring they are prominently visible, easy to understand, and updated in real time.
  4. Incorporate a notification system within the app to alert the instructor of any changes in headset status, especially disconnections, with optional auditory or vibratory alerts.
  5. Offer basic troubleshooting steps or quick reconnect options within the app for the instructor to utilize in case a headset gets disconnected.
  6. Ensure the secure and reliable transmission of status data between the server and the app, preventing any miscommunication or data breaches.
  7. Conduct extensive integration testing to validate the accurate and real-time reflection of headset statuses on the instructor app, ensuring consistency and reliability.
  8. Develop comprehensive documentation and possibly conduct brief training sessions for instructors to familiarize them with the headset status monitoring functionality and its usage.
  9. Set up a mechanism for collecting and analyzing feedback from instructors regarding the headset status indicators, using insights gained for continuous improvement and refinement of the feature.

### **_9\. As an instructor, I want to toggle a transparency (pass-through) mode on student headsets to allow them to briefly see their real-world environment so that they are aware of their surroundings when necessary._**

* _Definition of Done:_ 
  * The instructor can easily toggle the transparency mode for all student headsets via the instructor app, without student intervention.
  * When activated, the transparency mode provides clear and immediate visibility of the real-world environment to students through their headsets.
  * The toggle functionality is reliable, with minimal latency between the instructor’s command and the activation/deactivation of transparency mode on student devices.
  * Transparency mode can be deactivated, returning students to the VR experience seamlessly.
* _Tasks:_
  1. Investigate the API documentation for the Oculus Quest 3 to understand how to interact with the transparency (pass-through) mode programmatically.
  2. Implement the functionality on the headset application (developed in Unity) that allows the toggling of transparency mode based on received commands.
  3. Set up communication between the instructor's app and the student headsets to send and receive the transparency mode toggle commands securely and efficiently.
  4. Develop a user-friendly toggle button or switch on the instructor app that sends the command to activate or deactivate transparency mode on all connected headsets.
  5. Optimize the system to ensure minimal latency between the instructor's command and the change in transparency mode on student headsets.
  6. Implement a feature whereby the instructor receives immediate feedback on the app regarding the success or failure of the transparency mode toggle operation, along with the status of each headset.
  7. Ensure that the toggling process is safe, with no risk of causing discomfort or disorientation to students.
  8. Have instructors and students test the feature in real-world scenarios to ensure it meets the needs and expectations of both parties.
  9. Write comprehensive documentation outlining how the transparency mode toggle feature works, including troubleshooting steps and safety guidelines for instructors.

## Expanded Content Creator User Stories: 

### **_10\. As a content creator, I want to use a well-defined schema to simplify the process of creating configurations for the software stack._**

* _Definition of Done:_ 
  * An OpenAPI schema is established and documented, outlining the required fields, data types, and structure for creating configurations.
  * Content creators can easily create new configurations using the OpenAPI schema, with minimal chances of errors or compatibility issues.
  * Configurations created using the OpenAPI schema are seamlessly integrated into the Unity software stack, working efficiently with Oculus Quest 2 and 3 headsets.
  * The OpenAPI schema is flexible enough to accommodate future updates or changes in the software stack or hardware without significant rework.
* _Tasks:_
  1. Design a well-defined OpenAPI schema that outlines how configurations should be structured for easy integration into the Unity software stack for Oculus Quest 2 and 3.
  2. Produce detailed documentation explaining the OpenAPI schema’s structure, usage guidelines, and any constraints or limitations it may have.
  3. Develop a template based on the schema to assist content creators in generating configurations swiftly and accurately.
  4. Create a validation tool that checks new configurations against the OpenAPI schema to identify and report any errors or inconsistencies.
  5. Test configurations created using the OpenAPI schema with the Unity software stack and Oculus Quest 3 headsets to confirm compatibility and functionality.

# **Release Plan / Sprint Goals**

* [Sprint 0 - Planning](https://gitlab.cs.unh.edu/groups/mobile-vr-lab/-/milestones/1#tab-issues)
  * Artifact(s):
    * Planning and Initial Design Docs
  * Description / Sprint Goal
    * Develop charter, stories, design plans, QA/QC plans.
* [Sprint 1 - Preparation](https://gitlab.cs.unh.edu/groups/mobile-vr-lab/-/milestones/2#tab-issues)
  * Artifact(s):
    * Technology Selection and Learning Artifacts
  * Description / Sprint Goal
    * Select appropriate technologies for development, learn and understand selected technologies, and finalize the learning objectives.
* [Sprint 2 - MVP](https://gitlab.cs.unh.edu/groups/mobile-vr-lab/-/milestones/3#tab-issues)
  * Artifact(s):
    * Increment 1 - MVP
  * Description / Sprint Goal
    * Develop and demo the MVP with features that allow students to navigate using hand gestures, with the instructor able to see real-time connectivity statuses of students' headsets.
* [Sprint 3 - End of Semester](https://gitlab.cs.unh.edu/groups/mobile-vr-lab/-/milestones/4#tab-issues)
  * Artifact(s):
    * Increment 2, Increment 3, Tech Doc


    * Description / Sprint Goal
      * Develop additional features based on user stories, conduct thorough testing, and make necessary adjustments based on feedback. Finalize development, conduct final testing, and create documentation for end-users and technical documentation for future development and maintenance.

# Work Breakdown Structure

Initiative: _Mobile VR Lab Instructor App_

* Epic: [Planning / Initial Designs](https://gitlab.cs.unh.edu/groups/mobile-vr-lab/-/epics/1 "Planning / Initial Design")
  * Stories / Features: N/A
  * Tasks:
    * [Charter](https://gitlab.cs.unh.edu/mobile-vr-lab/documentation/-/issues/1 "Charter")
    * [Stories](https://gitlab.cs.unh.edu/mobile-vr-lab/documentation/-/issues/2 "Complete Personas for User Stories Document")
    * [Sequence Diagrams](https://gitlab.cs.unh.edu/mobile-vr-lab/android-app/-/issues/4 "Sequence Diagrams")
* Epic: [Preparation](https://gitlab.cs.unh.edu/groups/mobile-vr-lab/-/epics/2 "Preparation")
  * Stories / Features:
    * [Hand Gesture Navigation](https://gitlab.cs.unh.edu/groups/mobile-vr-lab/-/epics/7 "Preparation - Hand Gesture Navigation")
      * Tasks:
        * [Define and document navigation and interaction gestures.](https://gitlab.cs.unh.edu/mobile-vr-lab/android-app/-/issues/5 "Hand Gesture Navigation - Define and document navigation and interaction gestures.")
    * [Virtual Boundaries](https://gitlab.cs.unh.edu/groups/mobile-vr-lab/-/epics/8 "Preparation - Virtual Boundaries")
      * Tasks:
        * [Prepare instructional materials on the virtual boundary system.](https://gitlab.cs.unh.edu/mobile-vr-lab/android-app/-/issues/6 "Virtual Boundaries - Prepare instructional materials on the virtual boundary system")
    * [Real-time Scene Viewing](https://gitlab.cs.unh.edu/groups/mobile-vr-lab/-/epics/9 "Preparation - Real-time Scene Viewing") 
      * Tasks:
        * [Define data structure for scenes.](https://gitlab.cs.unh.edu/mobile-vr-lab/android-app/-/issues/7 "Real-time Scene Viewing - Define data structure for scene")
    * [Script Loading](https://gitlab.cs.unh.edu/groups/mobile-vr-lab/-/epics/10 "Preparation - Script Loading")
      * Tasks:
        * [Define and document standardized format for script files.](https://gitlab.cs.unh.edu/mobile-vr-lab/android-app/-/issues/8 "Script Loading - Define and document standardized format for script files.")
    * [Interactive Script](https://gitlab.cs.unh.edu/groups/mobile-vr-lab/-/epics/11 "Preparation - Interactive Script")
      * Tasks:
        * [Define possible actions triggered within VR exhibits.](https://gitlab.cs.unh.edu/mobile-vr-lab/android-app/-/issues/9 "Interactive Script - Define possible actions triggered within VR exhibits")
    * [Transparency Mode Toggle](https://gitlab.cs.unh.edu/groups/mobile-vr-lab/-/epics/12 "Preparation - Transparency Mode Toggle")
      * Tasks: 
        * [Investigate Oculus Quest 3 API for transparency mode control.](https://gitlab.cs.unh.edu/mobile-vr-lab/android-app/-/issues/10 "Transparency Mode Toggle - Investigate Oculus Quest 3 API for transparency mode control")
* Epic: [MVP](https://gitlab.cs.unh.edu/groups/mobile-vr-lab/-/epics/6 "MVP")
  * Stories / Features:
    * [Real-time Scene Viewing](https://gitlab.cs.unh.edu/groups/mobile-vr-lab/-/epics/13 "MVP - Real-time Scene Viewing") 
      * Tasks:
        * [Implement a real-time updating mechanism for current scenes.](https://gitlab.cs.unh.edu/mobile-vr-lab/android-app/-/issues/11 "Real-time Scene Viewing - Implement a real-time updating mechanism for current scenes.")
        * [Implement a UI component to display scene info.](https://gitlab.cs.unh.edu/mobile-vr-lab/android-app/-/issues/12 "Real-time Scene Viewing -  Implement a UI component to display scene info")
    * [Scene Navigation Control](https://gitlab.cs.unh.edu/groups/mobile-vr-lab/-/epics/14 "MVP - Scene Navigation Control")
      * Tasks:
        * [Develop UI elements for scene navigation in the app.](https://gitlab.cs.unh.edu/mobile-vr-lab/android-app/-/issues/13 "Scene Navigation Control - Develop UI elements for scene navigation in the app")
        * [Implement functionality to send navigation requests to the server.](https://gitlab.cs.unh.edu/mobile-vr-lab/android-app/-/issues/14 "Scene Navigation Control - Implement functionality to send navigation requests to the server")
        * [Develop a mechanism to broadcast scene changes to student headsets.](https://gitlab.cs.unh.edu/mobile-vr-lab/android-app/-/issues/15 "Scene Navigation Control - Develop a mechanism to broadcast scene changes to student headsets")
        * [Implement confirmation system for successful scene transitions.](https://gitlab.cs.unh.edu/mobile-vr-lab/android-app/-/issues/16 "Scene Navigation Control - Implement confirmation system for successful scene transition")
    * [Attention Mode](https://gitlab.cs.unh.edu/groups/mobile-vr-lab/-/epics/15 "MVP - Attention Mode")
      * Tasks:
        * [Design "attention mode" toggle button on instructor app.](https://gitlab.cs.unh.edu/mobile-vr-lab/android-app/-/issues/17 'Attention Mode - Design "attention mode" toggle button on instructor app')
    * [Script Loading](https://gitlab.cs.unh.edu/groups/mobile-vr-lab/-/epics/16 "MVP - Script Loading")
      * Tasks:
        * [Design system to map script files to VR exhibits.](https://gitlab.cs.unh.edu/mobile-vr-lab/android-app/-/issues/18 "Script Loading - Design system to map script files to VR exhibits")
        * [Implement script-fetching mechanism in app.](https://gitlab.cs.unh.edu/mobile-vr-lab/android-app/-/issues/19 "Script Loading - Implement script-fetching mechanism in app")
        * [Integrate responsive text display component in app.](https://gitlab.cs.unh.edu/mobile-vr-lab/android-app/-/issues/20 "Script Loading - Integrate responsive text display component in app")
    * [Interactive Script](https://gitlab.cs.unh.edu/groups/mobile-vr-lab/-/epics/17 "MVP - Interactive Script")
      * Tasks:
        * [Develop and document syntax within scripts for clickable elements.](https://gitlab.cs.unh.edu/mobile-vr-lab/android-app/-/issues/21 "Interactive Script - Develop and document syntax within scripts for clickable elements")
        * [Implement script parser to render clickable elements.](https://gitlab.cs.unh.edu/mobile-vr-lab/android-app/-/issues/22 "Interactive Script - Implement script parser to render clickable elements")
        * [Design and implement a user-friendly interface for interactive scripts.](https://gitlab.cs.unh.edu/mobile-vr-lab/android-app/-/issues/23 "Interactive Script - Design and implement a user-friendly interface for interactive scripts")
    * [Headset Connectivity Status](https://gitlab.cs.unh.edu/groups/mobile-vr-lab/-/epics/18 "MVP - Headset Connectivity Status")
      * Tasks:
        * [Implement server-side headset connectivity monitoring.](https://gitlab.cs.unh.edu/mobile-vr-lab/android-app/-/issues/28 "Headset Connectivity Status - Implement server-side headset connectivity monitoring")
        * [Develop intuitive status indicators on the instructor app.](https://gitlab.cs.unh.edu/mobile-vr-lab/android-app/-/issues/24 "Headset Connectivity Status - Develop intuitive status indicators on the instructor app")
        * [Offer troubleshooting options for disconnected headsets.](https://gitlab.cs.unh.edu/mobile-vr-lab/android-app/-/issues/25 "Headset Connectivity Status - Offer troubleshooting options for disconnected headsets")
    * [Transparency Mode Toggle](https://gitlab.cs.unh.edu/groups/mobile-vr-lab/-/epics/19 "MVP - Transparency Mode Toggle")
      * Tasks:
        * [Set up communication for transparency mode commands.](https://gitlab.cs.unh.edu/mobile-vr-lab/android-app/-/issues/26 "Transparency Mode Toggle - Set up communication for transparency mode commands")
        * [Develop a toggle button on the instructor app.](https://gitlab.cs.unh.edu/mobile-vr-lab/android-app/-/issues/27 "Transparency Mode Toggle Feature - Develop a toggle button on the instructor app") 

_Initiative: Mobile VR Lab Unity Application_

* Epic: [Planning / Initial Designs](https://gitlab.cs.unh.edu/groups/mobile-vr-lab/-/epics/1 "Planning / Initial Design")
  * Stories / Features: N/A
  * Tasks:
    * [Charter](https://gitlab.cs.unh.edu/mobile-vr-lab/documentation/-/issues/1 "Charter")
    * [Stories](https://gitlab.cs.unh.edu/mobile-vr-lab/documentation/-/issues/2 "Complete Personas for User Stories Document")
    * [Sequence Diagrams](https://gitlab.cs.unh.edu/mobile-vr-lab/android-app/-/issues/4 "Sequence Diagrams")
  * Epic: [Preparation](https://gitlab.cs.unh.edu/groups/mobile-vr-lab/-/epics/2 "Preparation")
* Epic: [Preparation](https://gitlab.cs.unh.edu/groups/mobile-vr-lab/-/epics/2 "Preparation")
  * Stories / Features:
    * [Hand Gesture Navigation](https://gitlab.cs.unh.edu/groups/mobile-vr-lab/-/epics/7 "Preparation - Hand Gesture Navigation")
      * Tasks:
        * [Research VR hand gestures in Unity and Unreal Engine.](https://gitlab.cs.unh.edu/mobile-vr-lab/vr-headset-application-unity/-/issues/1 "Hand Gesture Navigation - Research VR hand gestures in Unity and Unreal Engine")
    * [Virtual Boundaries](https://gitlab.cs.unh.edu/groups/mobile-vr-lab/-/epics/8 "Preparation - Virtual Boundaries")
      * Tasks:
        * [Research methods for dynamic virtual boundaries in Unity](https://gitlab.cs.unh.edu/mobile-vr-lab/vr-headset-application-unity/-/issues/2 "Virtual Boundaries - Research methods for dynamic virtual boundaries in Unity")
* Epic: [MVP](https://gitlab.cs.unh.edu/groups/mobile-vr-lab/-/epics/6 "MVP")
  * Stories / Features:
    * [Hand Gesture Navigation](https://gitlab.cs.unh.edu/groups/mobile-vr-lab/-/epics/20 "MVP - Hand Gesture Navigation")
      * Tasks:
        * [Implement a gesture recognition library in the VR application.](https://gitlab.cs.unh.edu/mobile-vr-lab/vr-headset-application-unity/-/issues/3 "Hand Gesture Navigation - Implement a gesture recognition library in the VR application")
        * [Develop a tutorial for students on gesture controls.](https://gitlab.cs.unh.edu/mobile-vr-lab/vr-headset-application-unity/-/issues/4 "Hand Gesture Navigation -  Develop a tutorial for students on gesture controls")
    * [Virtual Boundaries](https://gitlab.cs.unh.edu/groups/mobile-vr-lab/-/epics/21 "MVP - Virtual Boundaries")
      * Tasks:
        * [Develop an algorithm for translating physical space to virtual environment](https://gitlab.cs.unh.edu/mobile-vr-lab/vr-headset-application-unity/-/issues/5 "Virtual Boundaries - Develop an algorithm for translating physical space to virtual environment").
        * [Design visual cues for virtual boundaries.](https://gitlab.cs.unh.edu/mobile-vr-lab/vr-headset-application-unity/-/issues/6 "Virtual Boundaries - Design visual cues for virtual boundaries")
        * [Implement feedback for approaching or crossing boundaries.](https://gitlab.cs.unh.edu/mobile-vr-lab/vr-headset-application-unity/-/issues/7 "Virtual Boundaries - Implement feedback for approaching or crossing boundaries")
        * [Develop a calibration tool for configuring virtual boundaries.](https://gitlab.cs.unh.edu/mobile-vr-lab/vr-headset-application-unity/-/issues/8 "Virtual Boundaries - Develop a calibration tool for configuring virtual boundaries")
    * [Real-time Scene Viewing](https://gitlab.cs.unh.edu/groups/mobile-vr-lab/-/epics/13 "MVP - Real-time Scene Viewing") 
      * Tasks:
        * [Develop a server feature to broadcast active scenes to the instructor app.](https://gitlab.cs.unh.edu/mobile-vr-lab/vr-headset-application-unity/-/issues/9 "Real-time Scene Viewing  - Develop a server feature to broadcast active scenes to the instructor app")
    * [Scene Navigation Control](https://gitlab.cs.unh.edu/groups/mobile-vr-lab/-/epics/14 "MVP - Scene Navigation Control")
      * Tasks:
        * [Implement backend logic for scene change requests.](https://gitlab.cs.unh.edu/mobile-vr-lab/vr-headset-application-unity/-/issues/10 "Scene Navigation Control - Implement backend logic for scene change requests")
    * [Attention Mode](https://gitlab.cs.unh.edu/groups/mobile-vr-lab/-/epics/15 "MVP - Attention Mode")
      * Tasks:
        * [Develop backend functionality for “attention mode” activation.](https://gitlab.cs.unh.edu/mobile-vr-lab/vr-headset-application-unity/-/issues/11 "Attention Mode - Develop backend functionality for “attention mode” activation")
        * [Implement a responsive feature on student headset applications.](https://gitlab.cs.unh.edu/mobile-vr-lab/vr-headset-application-unity/-/issues/12 "Attention Mode - Implement a responsive feature on student headset applications")
        * [Integrate visual/auditory signals for "attention mode".](https://gitlab.cs.unh.edu/mobile-vr-lab/vr-headset-application-unity/-/issues/13 'Attention Mode - Integrate visual/auditory signals for "attention mode"')
    * [Interactive Script](https://gitlab.cs.unh.edu/groups/mobile-vr-lab/-/epics/17 "MVP - Interactive Script")
      * Tasks:
        * [Develop a mapping mechanism for script elements and actions.](https://gitlab.cs.unh.edu/mobile-vr-lab/vr-headset-application-unity/-/issues/14 "Interactive Script - Develop mapping mechanism for script elements and actions")
    * [Headset Connectivity Status](https://gitlab.cs.unh.edu/groups/mobile-vr-lab/-/epics/18 "MVP - Headset Connectivity Status")
      * Tasks:
        * [Integrate and update status indicators in real-time.](https://gitlab.cs.unh.edu/mobile-vr-lab/vr-headset-application-unity/-/issues/15 "Headset Connectivity Status - Integrate and update status indicators in real time")
        * [Incorporate notification system for connectivity changes.](https://gitlab.cs.unh.edu/mobile-vr-lab/vr-headset-application-unity/-/issues/16 "Headset Connectivity Status - Incorporate notification system for connectivity changes")
    * [Transparency Mode Toggle](https://gitlab.cs.unh.edu/groups/mobile-vr-lab/-/epics/19 "MVP - Transparency Mode Toggle")
      * Tasks:
        * [Implement transparency toggle feature on headset application.](https://gitlab.cs.unh.edu/mobile-vr-lab/vr-headset-application-unity/-/issues/17 "Transparency Mode Toggle - Implement transparency toggle feature on headset application.")
* Epic: [Tech-Doc](https://gitlab.cs.unh.edu/groups/mobile-vr-lab/-/epics/4 "Tech-Doc")
  * Stories / Features:
    * [OpenApi Schema](https://gitlab.cs.unh.edu/groups/mobile-vr-lab/-/epics/22 "Tech Doc - OpenApi Schema")
      * Tasks:
        * [Produce documentation for the schema.](https://gitlab.cs.unh.edu/mobile-vr-lab/vr-headset-application-unity/-/issues/18 "OpenApi Schema - Produce documentation for the schema")
        * [Develop a configuration template based on the schema.](https://gitlab.cs.unh.edu/mobile-vr-lab/vr-headset-application-unity/-/issues/19 "OpenApi Schema - Develop a configuration template based on the schema")
        * [Test configurations with Unity stack and Oculus Quest 3.](https://gitlab.cs.unh.edu/mobile-vr-lab/vr-headset-application-unity/-/issues/20 "OpenApi Schema - Test configurations with Unity stack and Oculus Quest 3")